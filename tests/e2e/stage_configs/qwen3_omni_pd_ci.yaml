# Stage config for Qwen3-Omni-MoE with PD (Prefill-Decode) disaggregation
# CI variant: uses load_format: dummy so tests can run without real weights.
#
# Stage 0: Thinker Prefill (prompt processing, KV producer)
# Stage 1: Thinker Decode  (token generation, KV consumer)
# Stage 2: Talker           (text embeddings -> RVQ codec codes)
# Stage 3: Code2Wav         (RVQ codes -> audio waveform)
#
# Requires 3x GPUs: GPU 0 = prefill, GPU 1 = decode, GPU 2 = talker + code2wav
# Both prefill and decode stages MUST use the same tensor_parallel_size.

async_chunk: false
stage_args:
  - stage_id: 0
    stage_type: llm
    is_prefill_only: true
    runtime:
      devices: "0"
      max_batch_size: 5
    engine_args:
      model_stage: thinker
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.9
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      max_model_len: 32768
      hf_config_name: thinker_config
      tensor_parallel_size: 1
      load_format: dummy
      kv_transfer_config:
        kv_connector: "MooncakeConnector"
        kv_role: "kv_producer"
        kv_rank: 0
        kv_parallel_size: 2
        engine_id: "omni-thinker-prefill"
        kv_connector_extra_config:
          mooncake_bootstrap_port: 25201
    final_output: false
    is_comprehension: true
    default_sampling_params:
      temperature: 0.4
      top_p: 0.9
      top_k: 1
      max_tokens: 100
      seed: 42
      detokenize: True
      repetition_penalty: 1.05

  - stage_id: 1
    stage_type: llm
    is_decode_only: true
    runtime:
      devices: "1"
      max_batch_size: 5
    engine_args:
      model_stage: thinker
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.9
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      max_model_len: 32768
      hf_config_name: thinker_config
      tensor_parallel_size: 1
      load_format: dummy
      kv_transfer_config:
        kv_connector: "MooncakeConnector"
        kv_role: "kv_consumer"
        kv_rank: 1
        kv_parallel_size: 2
        engine_id: "omni-thinker-decode"
        kv_connector_extra_config:
          mooncake_bootstrap_port: 25202
    engine_input_source: [0]
    final_output: true
    final_output_type: text
    is_comprehension: true
    default_sampling_params:
      temperature: 0.4
      top_p: 0.9
      top_k: 1
      max_tokens: 100
      seed: 42
      detokenize: True
      repetition_penalty: 1.05

  - stage_id: 2
    stage_type: llm
    runtime:
      devices: "2"
      max_batch_size: 5
    engine_args:
      model_stage: talker
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.6
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      max_model_len: 32768
      distributed_executor_backend: "mp"
      hf_config_name: talker_config
      load_format: dummy
    engine_input_source: [1]
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.qwen3_omni.thinker2talker
    default_sampling_params:
      temperature: 0.9
      top_k: 50
      max_tokens: 1000
      seed: 42
      detokenize: False
      repetition_penalty: 1.05
      stop_token_ids: [2150]

  - stage_id: 3
    stage_type: llm
    runtime:
      devices: "2"
      max_batch_size: 1
    engine_args:
      model_stage: code2wav
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: generation
      scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
      enforce_eager: true
      trust_remote_code: true
      enable_prefix_caching: false
      engine_output_type: audio
      gpu_memory_utilization: 0.1
      distributed_executor_backend: "mp"
      max_num_batched_tokens: 100000
      hf_config_name: thinker_config
      async_scheduling: false
      load_format: dummy
    engine_input_source: [2]
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.qwen3_omni.talker2code2wav
    final_output: true
    final_output_type: audio
    default_sampling_params:
      temperature: 0.0
      top_p: 1.0
      top_k: -1
      max_tokens: 2000
      seed: 42
      detokenize: True
      repetition_penalty: 1.1

# Runtime edges
runtime:
  enabled: true
  defaults:
    window_size: -1
    max_inflight: 1

  connectors:
    shared_memory_connector:
      name: SharedMemoryConnector
      extra:
        shm_threshold_bytes: 65536

  edges:
    - from: 0
      to: 1
      window_size: -1
    - from: 1
      to: 2
      window_size: -1
    - from: 2
      to: 3
      window_size: -1
