# Stage config for Qwen3-Omni-MoE with Prefill-Decode disaggregation
#
# Splits the thinker stage into separate prefill and decode instances.
# The prefill stage processes prompts and transfers KV cache to the
# decode stage via vLLM's native KV connector (e.g., MooncakeConnector).
#
# Stage 0: Thinker Prefill (prompt processing, KV producer)
# Stage 1: Thinker Decode  (token generation, KV consumer)
# Stage 2: Talker           (text embeddings -> RVQ codec codes)
# Stage 3: Code2Wav         (RVQ codes -> audio waveform)
#
# Requirements:
#   - A supported KV connector (MooncakeConnector, NixlConnector, etc.)
#   - Prefill and decode stages must be able to communicate via the connector
#   - Mooncake transfer engine must be installed if using MooncakeConnector
#
# The orchestrator overrides max_tokens=1 for the prefill stage so it
# performs only prompt processing + KV save, then the decode stage loads
# the KV cache and generates the full response.
#
# engine_id values must be set explicitly so the orchestrator can tell the
# decode engine where to pull KV from (the prefill engine's identity).
#
# IMPORTANT: MooncakeConnector does not support heterogeneous TP sizes.
# Both prefill and decode stages MUST use the same tensor_parallel_size.
# If the thinker model requires TP=2, set both stages to TP=2 and
# allocate 2 GPUs for each (e.g. devices "0,1" and "2,3", 5 GPUs total).
#
# Example layout on 3x H100-80G GPUs (TP=1 for both):
#   GPU 0: Thinker Prefill
#   GPU 1: Thinker Decode
#   GPU 2: Talker + Code2Wav

async_chunk: false
stage_args:
  - stage_id: 0
    stage_type: llm
    is_prefill_only: true
    runtime:
      devices: "0"
      max_batch_size: 16
    engine_args:
      model_stage: thinker
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.9
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      hf_config_name: thinker_config
      tensor_parallel_size: 1
      kv_transfer_config:
        kv_connector: "MooncakeConnector"
        kv_role: "kv_producer"
        kv_rank: 0
        kv_parallel_size: 2
        engine_id: "omni-thinker-prefill"
        kv_connector_extra_config:
          mooncake_bootstrap_port: 25201
    final_output: false
    is_comprehension: true
    default_sampling_params:
      temperature: 0.4
      top_p: 0.9
      top_k: 1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.05

  - stage_id: 1
    stage_type: llm
    is_decode_only: true
    runtime:
      devices: "1"
      max_batch_size: 64
    engine_args:
      model_stage: thinker
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.9
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      hf_config_name: thinker_config
      tensor_parallel_size: 1
      kv_transfer_config:
        kv_connector: "MooncakeConnector"
        kv_role: "kv_consumer"
        kv_rank: 1
        kv_parallel_size: 2
        engine_id: "omni-thinker-decode"
        kv_connector_extra_config:
          mooncake_bootstrap_port: 25202
    engine_input_source: [0]
    final_output: true
    final_output_type: text
    is_comprehension: true
    default_sampling_params:
      temperature: 0.4
      top_p: 0.9
      top_k: 1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.05

  - stage_id: 2
    stage_type: llm
    runtime:
      devices: "2"
      max_batch_size: 64
    engine_args:
      model_stage: talker
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.6
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      distributed_executor_backend: "mp"
      hf_config_name: talker_config
    engine_input_source: [1]
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.qwen3_omni.thinker2talker
    default_sampling_params:
      temperature: 0.9
      top_k: 50
      max_tokens: 4096
      seed: 42
      detokenize: False
      repetition_penalty: 1.05
      stop_token_ids: [2150]

  - stage_id: 3
    stage_type: llm
    runtime:
      devices: "2"
      max_batch_size: 1
    engine_args:
      model_stage: code2wav
      model_arch: Qwen3OmniMoeForConditionalGeneration
      worker_type: generation
      scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
      enforce_eager: true
      trust_remote_code: true
      async_scheduling: false
      enable_prefix_caching: false
      engine_output_type: audio
      gpu_memory_utilization: 0.1
      distributed_executor_backend: "mp"
      max_num_batched_tokens: 1000000
      hf_config_name: thinker_config
    engine_input_source: [2]
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.qwen3_omni.talker2code2wav
    final_output: true
    final_output_type: audio
    default_sampling_params:
      temperature: 0.0
      top_p: 1.0
      top_k: -1
      max_tokens: 65536
      seed: 42
      detokenize: True
      repetition_penalty: 1.1

# Runtime edges
runtime:
  enabled: true
  defaults:
    window_size: -1
    max_inflight: 1

  connectors:
    shared_memory_connector:
      name: SharedMemoryConnector
      extra:
        shm_threshold_bytes: 65536

  edges:
    - from: 0
      to: 1
      window_size: -1
    - from: 1
      to: 2
      window_size: -1
    - from: 2
      to: 3
      window_size: -1
